% Run value iteration to solve a linear MDP.
function [v,q,p,logp] = td_learning(mdp_data,r,qinit)

states = mdp_data.states;
actions = mdp_data.actions;
CONV_THRESH = 1e-4;

% Compute log state partition function V.
if (nargin == 3)
    q = qinit;
else
    q = zeros(states,actions);
end;
r = repmat(r,1,actions);
alpha = 0.1;
diff = 1.0;

while diff >= CONV_THRESH,
    % Initialize new v.
    qp = q;
    
    % Compute q function.
    q = qp+ alpha*(r+ mdp_data.discount)                      r + mdp_data.discount*sum(mdp_data.sa_p.*vp(mdp_data.sa_s),3);
    
    qp(mdp_data.sa_s, )

    % Compute softmax.
    v = maxentsoftmax(q);
    
    diff = max(abs(v-vp));
end;

% Compute policy.
logp = q - repmat(v,1,actions);
p = exp(logp);