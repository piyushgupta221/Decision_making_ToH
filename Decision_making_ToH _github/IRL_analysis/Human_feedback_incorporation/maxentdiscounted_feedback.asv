% Compute MaxEnt objective and gradient. Discounted infinite-horizon version.
function val = maxentdiscounted_feedback(r,F,muE,mu_sa,mdp_data,initD,laplace_prior, model_num)

if nargin < 7,
    laplace_prior = 0;
end;

% Compute constants.
actions = mdp_data.actions;

% Convert to full reward.
wts = r;

[row,~,~] = find(F); %Finds non zero indices
rew = zeros(size(F,1),1);

if model_num==0
    rew(row) = r;
    r = F*rew;
    




[~,~,policy,logpolicy] = linearvalueiteration_without_feedback(mdp_data,repmat(r,1,actions));


%[~,~,policy,logpolicy] = td_learning(mdp_data,repmat(r,1,actions));

% Compute value by adding up log example probabilities.
val = sum(sum(logpolicy.*mu_sa));   % This is computing a mean Q value by averaging with the probability of observing (s,a) 

% Add laplace prior.
if laplace_prior,
    val = val - laplace_prior*sum(abs(wts));
end;

% Invert for descent.
val = -val;